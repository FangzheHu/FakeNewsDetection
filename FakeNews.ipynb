{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label         0\n",
       "all_text    558\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['text'])\n",
    "df['all_text'] = df['title'] + ' ' + df['text']\n",
    "df = df.drop(columns=['id', 'author', 'title', 'text'])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label       0\n",
       "all_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop empty rows\n",
    "df = df.dropna(subset=['all_text'])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10387\n",
       "1     9816\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class distribution\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing:\n",
      "0    House Dem Aide: We Didn’t Even See Comey’s Let...\n",
      "1    FLYNN: Hillary Clinton, Big Woman on Campus - ...\n",
      "2    Why the Truth Might Get You Fired Why the Trut...\n",
      "3    15 Civilians Killed In Single US Airstrike Hav...\n",
      "4    Iranian woman jailed for fictional unpublished...\n",
      "Name: all_text, dtype: object\n",
      "After preprocessing:\n",
      "0    hous dem aid didnt even see comey letter jason...\n",
      "1    flynn hillari clinton big woman campu breitbar...\n",
      "2    truth might get fire truth might get fire octo...\n",
      "3    civilian kill singl us airstrik identifi video...\n",
      "4    iranian woman jail fiction unpublish stori wom...\n",
      "Name: all_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove non-letter characters\n",
    "    text = ''.join(c for c in text if c.isalpha() or c.isspace())\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize, alternative nltk.word_tokenize(text)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and perform stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    processed_tokens = [ps.stem(word) for word in tokens if not word in stop_words]\n",
    "    processed_tokens = ' '.join(processed_tokens)\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "X = df['all_text']\n",
    "y = df['label']\n",
    "print('Before preprocessing:')\n",
    "print(X.head())\n",
    "\n",
    "X = X.apply(preprocess_text)\n",
    "print('After preprocessing:')\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 55387)\t0.07173242381361301\n",
      "  (0, 110165)\t0.02699437729528575\n",
      "  (0, 37604)\t0.02249940634569755\n",
      "  (0, 106139)\t0.04211473998353424\n",
      "  (0, 13877)\t0.030489427496529307\n",
      "  (0, 21863)\t0.051460707432091565\n",
      "  (0, 11683)\t0.06930575625257465\n",
      "  (0, 54702)\t0.03196610940834586\n",
      "  (0, 21077)\t0.04162615888040291\n",
      "  (0, 60375)\t0.09875557695308286\n",
      "  (0, 85240)\t0.04521948827461712\n",
      "  (0, 60209)\t0.03330387632948736\n",
      "  (0, 106772)\t0.023161878629124222\n",
      "  (0, 35209)\t0.032701704408892134\n",
      "  (0, 89036)\t0.040815366752584556\n",
      "  (0, 23214)\t0.03564488824094397\n",
      "  (0, 52384)\t0.030037324283202847\n",
      "  (0, 112583)\t0.020104921246336477\n",
      "  (0, 62254)\t0.025899067970918507\n",
      "  (0, 106940)\t0.02850465330092847\n",
      "  (0, 43200)\t0.03586365346946216\n",
      "  (0, 13079)\t0.06086397061750317\n",
      "  (0, 117828)\t0.02740491416162256\n",
      "  (0, 16157)\t0.03215015448499058\n",
      "  (0, 20707)\t0.03508215678263701\n",
      "  :\t:\n",
      "  (16161, 95794)\t0.03999302533780579\n",
      "  (16161, 51452)\t0.05517254394847048\n",
      "  (16161, 84916)\t0.037328140751521625\n",
      "  (16161, 642)\t0.02690648738008708\n",
      "  (16161, 85783)\t0.05577656975984492\n",
      "  (16161, 101340)\t0.14388394800149637\n",
      "  (16161, 95694)\t0.0767276375768839\n",
      "  (16161, 59944)\t0.1581437883635774\n",
      "  (16161, 11629)\t0.04125988997386174\n",
      "  (16161, 80129)\t0.034926501246498015\n",
      "  (16161, 14620)\t0.050263996489415985\n",
      "  (16161, 85179)\t0.08677295013033863\n",
      "  (16161, 54702)\t0.036298029044007106\n",
      "  (16161, 106772)\t0.026300684029261837\n",
      "  (16161, 93798)\t0.021558250556117913\n",
      "  (16161, 59709)\t0.02347254429632668\n",
      "  (16161, 46078)\t0.054590032072562376\n",
      "  (16161, 49973)\t0.03642621679209473\n",
      "  (16161, 42440)\t0.02670204722631039\n",
      "  (16161, 82638)\t0.09512697228347883\n",
      "  (16161, 50915)\t0.04782182610274153\n",
      "  (16161, 15737)\t0.04623691408003359\n",
      "  (16161, 119735)\t0.094770619974766\n",
      "  (16161, 72825)\t0.13034552163642663\n",
      "  (16161, 107379)\t0.018048927068124755\n"
     ]
    }
   ],
   "source": [
    "# Create feature vectors using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 119829)\t0.02627291248951931\n",
      "  (0, 118403)\t0.025815869517282642\n",
      "  (0, 118254)\t0.28181362974230584\n",
      "  (0, 118213)\t0.015434901440427855\n",
      "  (0, 117828)\t0.02042838850107635\n",
      "  (0, 116952)\t0.044256212340370554\n",
      "  (0, 116498)\t0.016941422454450454\n",
      "  (0, 116319)\t0.01757079526693867\n",
      "  (0, 116206)\t0.028625458599034287\n",
      "  (0, 116107)\t0.09661170675656593\n",
      "  (0, 115961)\t0.03793188292184167\n",
      "  (0, 115693)\t0.11296839639653106\n",
      "  (0, 113275)\t0.11030360371256909\n",
      "  (0, 112760)\t0.05686472841346985\n",
      "  (0, 112583)\t0.029973539751411413\n",
      "  (0, 112479)\t0.03121783535696492\n",
      "  (0, 112211)\t0.05144910620149095\n",
      "  (0, 110186)\t0.0305218093881764\n",
      "  (0, 109942)\t0.03895073606858267\n",
      "  (0, 109535)\t0.03088641559364885\n",
      "  (0, 108873)\t0.46562745122257093\n",
      "  (0, 107845)\t0.05494933601013468\n",
      "  (0, 107811)\t0.025127030027319506\n",
      "  (0, 107569)\t0.07723118327715343\n",
      "  (0, 107538)\t0.07237361583192545\n",
      "  :\t:\n",
      "  (4040, 6975)\t0.01870452375035868\n",
      "  (4040, 6386)\t0.049176994007298476\n",
      "  (4040, 6323)\t0.020156096339345116\n",
      "  (4040, 6255)\t0.016198237191271187\n",
      "  (4040, 5730)\t0.01086830459238065\n",
      "  (4040, 5687)\t0.009724712263869402\n",
      "  (4040, 5612)\t0.007064357953648481\n",
      "  (4040, 5432)\t0.011206195882589452\n",
      "  (4040, 5356)\t0.00872109049771893\n",
      "  (4040, 5348)\t0.02748844622413032\n",
      "  (4040, 5270)\t0.016687505837984937\n",
      "  (4040, 4749)\t0.02151798058378269\n",
      "  (4040, 4713)\t0.009534452243642447\n",
      "  (4040, 4610)\t0.008184803679801265\n",
      "  (4040, 3994)\t0.009964662426276766\n",
      "  (4040, 3980)\t0.007579482348149167\n",
      "  (4040, 3521)\t0.019098953235128474\n",
      "  (4040, 3462)\t0.009478857635870787\n",
      "  (4040, 3392)\t0.023263865661486523\n",
      "  (4040, 3333)\t0.007661253184817464\n",
      "  (4040, 3202)\t0.01718786118388086\n",
      "  (4040, 3088)\t0.007495591098996887\n",
      "  (4040, 2431)\t0.017251025143028117\n",
      "  (4040, 595)\t0.014434417316616667\n",
      "  (4040, 310)\t0.008667213315724717\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9532293986636972\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8250433061123484\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9267508042563722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9673348181143281\n"
     ]
    }
   ],
   "source": [
    "# Train an SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', C=1, gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9638703291264539\n"
     ]
    }
   ],
   "source": [
    "# Train the Gradient Boosting classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8604305864884929\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors (KNN)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best parameters:  {'clf': SVC(C=1, kernel='linear'), 'clf__C': 1, 'clf__kernel': 'linear'}\n",
      "Best score:  0.9630613138641424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('clf', None)\n",
    "])\n",
    "\n",
    "# Define the parameters to search over\n",
    "parameters = [\n",
    "    {\n",
    "        'clf': [LogisticRegression(max_iter=3000)],\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__solver': ['lbfgs', 'liblinear'],\n",
    "    },\n",
    "    {\n",
    "        'clf': [MultinomialNB()],\n",
    "        'clf__alpha': [0.1, 1, 10],\n",
    "    },\n",
    "    {\n",
    "        'clf': [RandomForestClassifier()],\n",
    "        'clf__n_estimators': [10, 100, 500],\n",
    "    },\n",
    "    {\n",
    "        'clf': [SVC()],\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__kernel': ['linear', 'rbf'],\n",
    "    },\n",
    "    {\n",
    "        'clf': [GradientBoostingClassifier()],\n",
    "        'clf__n_estimators': [100, 200],\n",
    "        'clf__learning_rate': [0.01, 0.1, 1],\n",
    "    },\n",
    "    {\n",
    "        'clf': [KNeighborsClassifier()],\n",
    "        'clf__n_neighbors': [3, 5, 10],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the grid search\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best model's parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
