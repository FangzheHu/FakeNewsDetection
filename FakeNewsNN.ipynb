{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df = df.dropna(subset=['text'])\n",
    "df['all_text'] = df['title'] + ' ' + df['text']\n",
    "df = df.drop(columns=['id', 'author', 'title', 'text'])\n",
    "\n",
    "# Drop empty rows\n",
    "df = df.dropna(subset=['all_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove non-letter characters\n",
    "    text = ''.join(c for c in text if c.isalpha() or c.isspace())\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize, alternative nltk.word_tokenize(text)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and perform stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    processed_tokens = [ps.stem(word) for word in tokens if not word in stop_words]\n",
    "    processed_tokens = ' '.join(processed_tokens)\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "X = df['all_text']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8139     NY Times: Being a Sanctuary City ’Not Enough’ ...\n",
      "12358    What’s at Stake in Trump’s Proposed E.P.A. Cut...\n",
      "9562     Rick Rule: Broadcast Interview – Available Now...\n",
      "5979     Hillary Clinton Cancels Public Events And Vani...\n",
      "19059    Trump Declared The Winner By Jon Rappoport Bef...\n",
      "                               ...                        \n",
      "11625    Chaiwali, an Indian Restaurant That Feels Like...\n",
      "12329    Recipe: Mouth-Watering Cauliflower, Coconut oi...\n",
      "5553     Las cajetillas de tabaco emitirán música de Me...\n",
      "885      Open Borders Groups Gird for H-1B Fights The o...\n",
      "16251    Eighty Wealthy New Yorkers Ask State Governmen...\n",
      "Name: all_text, Length: 16162, dtype: object\n",
      "214620\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "print(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "\n",
    "# Convert text data to sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences to ensure uniform length\n",
    "max_length = 200\n",
    "train_pad = pad_sequences(training_sequences, maxlen=max_length, padding='post')\n",
    "test_pad = pad_sequences(testing_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 102 5913 3484 6030   24    1 9663  587    7    5 5553   10    1   90\n",
      " 3238 6750   36 1976 5631 7842   10    1 6914   36   42 1353   44  892\n",
      " 1077   26 1633    3 5082  324   15 2064 3995   38 2119 5284  863    2\n",
      " 1602 1022 6954   59  222  871   83   22 1022 3388    1  116    1  587\n",
      "    7  632 2063   62   22 4327    9 6750   63 1333 2222   38  105 2315\n",
      " 2265   11  539    7  228 9092    2   21   25 1147   20  274   75    1\n",
      "  101 1587 1708  980   49   66 1778    6    1   12    4 1330 4955   55\n",
      "  924  478 3125 1921  273  435    4 3680  708   28   50  324    2 1267\n",
      "  980    2  409 2173  111   31 1706    9  928    2 1188    9 9093  890\n",
      "   11    1  888   10    2  147    7  141 3388  683    3  632  928 2259\n",
      "   99   33   21 1472 1077  833  237 9275   10    1  850   11  265 1708\n",
      "   28  184  212  980    2 2802 1921    4 6077 2865  924 4436    1  811\n",
      " 1218    7  480    2 1022 6954  409  158    4 1871   42   22  388    1\n",
      "    9 1022 3388   11    1 7417  372    8    5 3890    9  243  635  377\n",
      "   81   10  197   20]\n"
     ]
    }
   ],
   "source": [
    "print(train_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "253/253 [==============================] - 95s 361ms/step - loss: 0.3760 - accuracy: 0.8433 - val_loss: 0.3150 - val_accuracy: 0.8792\n",
      "Epoch 2/10\n",
      "253/253 [==============================] - 91s 359ms/step - loss: 0.2136 - accuracy: 0.9309 - val_loss: 0.2528 - val_accuracy: 0.9112\n",
      "Epoch 3/10\n",
      "253/253 [==============================] - 91s 358ms/step - loss: 0.1406 - accuracy: 0.9570 - val_loss: 0.2366 - val_accuracy: 0.9047\n",
      "Epoch 4/10\n",
      "253/253 [==============================] - 91s 360ms/step - loss: 0.0946 - accuracy: 0.9707 - val_loss: 0.2712 - val_accuracy: 0.9178\n",
      "Epoch 5/10\n",
      "253/253 [==============================] - 91s 359ms/step - loss: 0.0617 - accuracy: 0.9809 - val_loss: 0.2169 - val_accuracy: 0.9329\n",
      "Epoch 6/10\n",
      "253/253 [==============================] - 91s 359ms/step - loss: 0.0471 - accuracy: 0.9863 - val_loss: 0.3386 - val_accuracy: 0.9235\n",
      "Epoch 7/10\n",
      "253/253 [==============================] - 91s 361ms/step - loss: 0.0495 - accuracy: 0.9864 - val_loss: 0.2592 - val_accuracy: 0.9280\n",
      "Epoch 8/10\n",
      "253/253 [==============================] - 92s 364ms/step - loss: 0.0494 - accuracy: 0.9852 - val_loss: 0.2352 - val_accuracy: 0.9305\n",
      "Epoch 8: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x152abd45760>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 128, input_length=max_length),\n",
    "    LSTM(128, dropout=0.2, return_sequences=True),\n",
    "    LSTM(64, dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_pad, y_train, validation_data=(test_pad, y_test), epochs=10, batch_size=64, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 6s 43ms/step - loss: 0.2352 - accuracy: 0.9305\n",
      "Loss: 0.235181\n",
      "Accuracy: 0.930463\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_pad, y_test)\n",
    "print('Loss: %f' % loss)\n",
    "print('Accuracy: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Freelook\\AppData\\Local\\Temp\\ipykernel_10400\\4084820254.py:18: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=64, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "Epoch 1/10\n",
      "127/127 [==============================] - 51s 381ms/step - loss: 0.5916 - accuracy: 0.6667 - val_loss: 0.6549 - val_accuracy: 0.7105\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 46s 365ms/step - loss: 0.5028 - accuracy: 0.7515 - val_loss: 0.4407 - val_accuracy: 0.7837\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 46s 366ms/step - loss: 0.3130 - accuracy: 0.8724 - val_loss: 0.3703 - val_accuracy: 0.8315\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 46s 365ms/step - loss: 0.2153 - accuracy: 0.9220 - val_loss: 0.2734 - val_accuracy: 0.8894\n",
      "Epoch 5/10\n",
      "127/127 [==============================] - 47s 371ms/step - loss: 0.1260 - accuracy: 0.9548 - val_loss: 0.2416 - val_accuracy: 0.9164\n",
      "Epoch 6/10\n",
      "127/127 [==============================] - 48s 378ms/step - loss: 0.0612 - accuracy: 0.9821 - val_loss: 0.2725 - val_accuracy: 0.9144\n",
      "Epoch 7/10\n",
      "127/127 [==============================] - 47s 369ms/step - loss: 0.0346 - accuracy: 0.9905 - val_loss: 0.2747 - val_accuracy: 0.9173\n",
      "Epoch 8/10\n",
      "127/127 [==============================] - 46s 365ms/step - loss: 0.0211 - accuracy: 0.9951 - val_loss: 0.3088 - val_accuracy: 0.9223\n",
      "Epoch 8: early stopping\n",
      "127/127 [==============================] - 9s 68ms/step - loss: 0.2731 - accuracy: 0.9279\n",
      "Epoch 1/10\n",
      "127/127 [==============================] - 49s 366ms/step - loss: 0.4294 - accuracy: 0.8062 - val_loss: 0.2785 - val_accuracy: 0.9005\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 46s 362ms/step - loss: 0.1901 - accuracy: 0.9402 - val_loss: 0.4122 - val_accuracy: 0.8948\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 47s 368ms/step - loss: 0.1967 - accuracy: 0.9404 - val_loss: 0.2409 - val_accuracy: 0.9169\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 49s 383ms/step - loss: 0.1278 - accuracy: 0.9604 - val_loss: 0.3286 - val_accuracy: 0.8931\n",
      "Epoch 5/10\n",
      "127/127 [==============================] - 51s 398ms/step - loss: 0.3723 - accuracy: 0.8477 - val_loss: 0.5134 - val_accuracy: 0.7330\n",
      "Epoch 6/10\n",
      "127/127 [==============================] - 50s 393ms/step - loss: 0.4186 - accuracy: 0.7942 - val_loss: 0.5066 - val_accuracy: 0.7347\n",
      "Epoch 6: early stopping\n",
      "127/127 [==============================] - 9s 64ms/step - loss: 0.4994 - accuracy: 0.7446\n",
      "Epoch 1/10\n",
      "127/127 [==============================] - 36s 253ms/step - loss: 0.6941 - accuracy: 0.4921 - val_loss: 0.6922 - val_accuracy: 0.5170\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 30s 239ms/step - loss: 0.6908 - accuracy: 0.5477 - val_loss: 0.6888 - val_accuracy: 0.6080\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 31s 242ms/step - loss: 0.6876 - accuracy: 0.6054 - val_loss: 0.6853 - val_accuracy: 0.6127\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 31s 243ms/step - loss: 0.6840 - accuracy: 0.6070 - val_loss: 0.6811 - val_accuracy: 0.6122\n",
      "Epoch 5/10\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.6795 - accuracy: 0.6076 - val_loss: 0.6758 - val_accuracy: 0.6125\n",
      "Epoch 6/10\n",
      "127/127 [==============================] - 31s 243ms/step - loss: 0.6739 - accuracy: 0.6074 - val_loss: 0.6692 - val_accuracy: 0.6125\n",
      "Epoch 7/10\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.6675 - accuracy: 0.6074 - val_loss: 0.6620 - val_accuracy: 0.6130\n",
      "Epoch 8/10\n",
      "127/127 [==============================] - 31s 245ms/step - loss: 0.6613 - accuracy: 0.6077 - val_loss: 0.6555 - val_accuracy: 0.6127\n",
      "Epoch 9/10\n",
      "127/127 [==============================] - 31s 244ms/step - loss: 0.6565 - accuracy: 0.6077 - val_loss: 0.6513 - val_accuracy: 0.6130\n",
      "Epoch 10/10\n",
      "127/127 [==============================] - 31s 245ms/step - loss: 0.6538 - accuracy: 0.6078 - val_loss: 0.6489 - val_accuracy: 0.6130\n",
      "127/127 [==============================] - 10s 73ms/step - loss: 0.6544 - accuracy: 0.6049\n",
      "Epoch 1/10\n",
      "127/127 [==============================] - 34s 244ms/step - loss: 0.6916 - accuracy: 0.5582 - val_loss: 0.6900 - val_accuracy: 0.6107\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 30s 237ms/step - loss: 0.6890 - accuracy: 0.6036 - val_loss: 0.6872 - val_accuracy: 0.6122\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 31s 242ms/step - loss: 0.6863 - accuracy: 0.6039 - val_loss: 0.6839 - val_accuracy: 0.6122\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.6829 - accuracy: 0.6043 - val_loss: 0.6798 - val_accuracy: 0.6127\n",
      "Epoch 5/10\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.6786 - accuracy: 0.6048 - val_loss: 0.6743 - val_accuracy: 0.6127\n",
      "Epoch 6/10\n",
      "127/127 [==============================] - 33s 259ms/step - loss: 0.6729 - accuracy: 0.6051 - val_loss: 0.6675 - val_accuracy: 0.6132\n",
      "Epoch 7/10\n",
      "127/127 [==============================] - 32s 253ms/step - loss: 0.6664 - accuracy: 0.6049 - val_loss: 0.6602 - val_accuracy: 0.6130\n",
      "Epoch 8/10\n",
      "127/127 [==============================] - 31s 248ms/step - loss: 0.6607 - accuracy: 0.6050 - val_loss: 0.6544 - val_accuracy: 0.6130\n",
      "Epoch 9/10\n",
      "127/127 [==============================] - 31s 241ms/step - loss: 0.6572 - accuracy: 0.6050 - val_loss: 0.6509 - val_accuracy: 0.6130\n",
      "Epoch 10/10\n",
      "127/127 [==============================] - 30s 240ms/step - loss: 0.6554 - accuracy: 0.6051 - val_loss: 0.6495 - val_accuracy: 0.6130\n",
      "127/127 [==============================] - 10s 74ms/step - loss: 0.6534 - accuracy: 0.6092\n",
      "Epoch 1/10\n",
      "127/127 [==============================] - 51s 377ms/step - loss: 0.4185 - accuracy: 0.8133 - val_loss: 0.2624 - val_accuracy: 0.8951\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 49s 386ms/step - loss: 0.2727 - accuracy: 0.9014 - val_loss: 0.2744 - val_accuracy: 0.8948\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 47s 373ms/step - loss: 0.1653 - accuracy: 0.9469 - val_loss: 0.2877 - val_accuracy: 0.8956\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 48s 380ms/step - loss: 0.1154 - accuracy: 0.9594 - val_loss: 0.3326 - val_accuracy: 0.9050\n",
      "Epoch 4: early stopping\n",
      "127/127 [==============================] - 10s 71ms/step - loss: 0.3242 - accuracy: 0.9081\n",
      "Epoch 1/10\n",
      "127/127 [==============================] - 54s 396ms/step - loss: 0.4510 - accuracy: 0.8013 - val_loss: 0.3161 - val_accuracy: 0.8854\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 51s 399ms/step - loss: 0.3154 - accuracy: 0.8744 - val_loss: 0.3310 - val_accuracy: 0.8661\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 49s 388ms/step - loss: 0.2137 - accuracy: 0.9297 - val_loss: 0.2696 - val_accuracy: 0.8924\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 49s 390ms/step - loss: 0.1069 - accuracy: 0.9677 - val_loss: 0.2458 - val_accuracy: 0.9176\n",
      "Epoch 5/10\n",
      "127/127 [==============================] - 49s 389ms/step - loss: 0.1081 - accuracy: 0.9687 - val_loss: 0.2756 - val_accuracy: 0.9176\n",
      "Epoch 6/10\n",
      "127/127 [==============================] - 50s 391ms/step - loss: 0.0751 - accuracy: 0.9817 - val_loss: 0.2867 - val_accuracy: 0.9129\n",
      "Epoch 7/10\n",
      "127/127 [==============================] - 49s 390ms/step - loss: 0.1230 - accuracy: 0.9602 - val_loss: 0.3875 - val_accuracy: 0.8770\n",
      "Epoch 7: early stopping\n",
      "127/127 [==============================] - 11s 80ms/step - loss: 0.3989 - accuracy: 0.8721\n",
      "Epoch 1/10\n",
      "127/127 [==============================] - 36s 259ms/step - loss: 0.6913 - accuracy: 0.5659 - val_loss: 0.6899 - val_accuracy: 0.6125\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 32s 253ms/step - loss: 0.6890 - accuracy: 0.5950 - val_loss: 0.6874 - val_accuracy: 0.6127\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 32s 254ms/step - loss: 0.6863 - accuracy: 0.6069 - val_loss: 0.6845 - val_accuracy: 0.6127\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 32s 253ms/step - loss: 0.6833 - accuracy: 0.6072 - val_loss: 0.6807 - val_accuracy: 0.6130\n",
      "Epoch 5/10\n",
      "127/127 [==============================] - 32s 253ms/step - loss: 0.6793 - accuracy: 0.6075 - val_loss: 0.6760 - val_accuracy: 0.6130\n",
      "Epoch 6/10\n",
      "127/127 [==============================] - 32s 254ms/step - loss: 0.6742 - accuracy: 0.6074 - val_loss: 0.6699 - val_accuracy: 0.6130\n",
      "Epoch 7/10\n",
      "127/127 [==============================] - 33s 257ms/step - loss: 0.6681 - accuracy: 0.6075 - val_loss: 0.6630 - val_accuracy: 0.6125\n",
      "Epoch 8/10\n",
      "127/127 [==============================] - 33s 258ms/step - loss: 0.6619 - accuracy: 0.6077 - val_loss: 0.6565 - val_accuracy: 0.6130\n",
      "Epoch 9/10\n",
      "127/127 [==============================] - 32s 254ms/step - loss: 0.6573 - accuracy: 0.6077 - val_loss: 0.6522 - val_accuracy: 0.6130\n",
      "Epoch 10/10\n",
      "127/127 [==============================] - 32s 253ms/step - loss: 0.6543 - accuracy: 0.6078 - val_loss: 0.6496 - val_accuracy: 0.6127\n",
      "127/127 [==============================] - 11s 80ms/step - loss: 0.6550 - accuracy: 0.6049\n",
      "Epoch 1/10\n",
      "127/127 [==============================] - 35s 255ms/step - loss: 0.6922 - accuracy: 0.5222 - val_loss: 0.6900 - val_accuracy: 0.5170\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 32s 252ms/step - loss: 0.6886 - accuracy: 0.5920 - val_loss: 0.6861 - val_accuracy: 0.6120\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.6847 - accuracy: 0.6038 - val_loss: 0.6816 - val_accuracy: 0.6122\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.6799 - accuracy: 0.6049 - val_loss: 0.6759 - val_accuracy: 0.6127\n",
      "Epoch 5/10\n",
      "127/127 [==============================] - 32s 252ms/step - loss: 0.6741 - accuracy: 0.6046 - val_loss: 0.6689 - val_accuracy: 0.6132\n",
      "Epoch 6/10\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.6674 - accuracy: 0.6050 - val_loss: 0.6613 - val_accuracy: 0.6135\n",
      "Epoch 7/10\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.6611 - accuracy: 0.6050 - val_loss: 0.6545 - val_accuracy: 0.6125\n",
      "Epoch 8/10\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.6569 - accuracy: 0.6050 - val_loss: 0.6504 - val_accuracy: 0.6132\n",
      "Epoch 9/10\n",
      "127/127 [==============================] - 32s 252ms/step - loss: 0.6544 - accuracy: 0.6050 - val_loss: 0.6480 - val_accuracy: 0.6135\n",
      "Epoch 10/10\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.6527 - accuracy: 0.6051 - val_loss: 0.6462 - val_accuracy: 0.6132\n",
      "127/127 [==============================] - 11s 82ms/step - loss: 0.6507 - accuracy: 0.6088\n",
      "Epoch 1/10\n",
      "253/253 [==============================] - 99s 376ms/step - loss: 0.3181 - accuracy: 0.8699 - val_loss: 0.2009 - val_accuracy: 0.9305\n",
      "Epoch 2/10\n",
      "253/253 [==============================] - 94s 373ms/step - loss: 0.1678 - accuracy: 0.9479 - val_loss: 0.2963 - val_accuracy: 0.8745\n",
      "Epoch 3/10\n",
      "253/253 [==============================] - 95s 375ms/step - loss: 0.1020 - accuracy: 0.9684 - val_loss: 0.1869 - val_accuracy: 0.9389\n",
      "Epoch 4/10\n",
      "253/253 [==============================] - 92s 363ms/step - loss: 0.1842 - accuracy: 0.9285 - val_loss: 0.2256 - val_accuracy: 0.9188\n",
      "Epoch 5/10\n",
      "253/253 [==============================] - 93s 366ms/step - loss: 0.0892 - accuracy: 0.9696 - val_loss: 0.2390 - val_accuracy: 0.9272\n",
      "Epoch 6/10\n",
      "253/253 [==============================] - 94s 373ms/step - loss: 0.0780 - accuracy: 0.9757 - val_loss: 0.2311 - val_accuracy: 0.9258\n",
      "Epoch 6: early stopping\n",
      "Best parameters:  {'dropout_rate': 0.3, 'optimizer': 'adam'}\n",
      "Best accuracy:  0.8900677561759949\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(dropout_rate=0.0, optimizer='adam'):\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, 128, input_length=max_length),\n",
    "        LSTM(128, dropout=dropout_rate, return_sequences=True),\n",
    "        LSTM(64, dropout=dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=64, verbose=1)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.2, 0.3],\n",
    "    'optimizer': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=2, verbose=1)\n",
    "\n",
    "# Fit the grid search\n",
    "grid_result = grid.fit(train_pad, y_train, validation_data=(test_pad, y_test), callbacks=[es])\n",
    "\n",
    "# Print the best parameters and best accuracy\n",
    "print(\"Best parameters: \", grid_result.best_params_)\n",
    "print(\"Best accuracy: \", grid_result.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
